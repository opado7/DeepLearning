{"cells":[{"cell_type":"markdown","metadata":{"id":"h-Gmtx7zQb6K"},"source":["## 딥러닝 과제\n","1. 11주차에 사용된 인물사진 이미지 데이터를 사용해 생성모델을 활용하여 가짜 인물 이미지를 생성하는 코드를 완성하여라.\n","2. 1번에서 완성한 코드를 실행하여 도출된 가짜를 2~3개를 제출하여라.<br>\n","최종적으로 1번 코드와 2번 이미지 결과를 각각 첨부하여 제출하여라."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9637,"status":"ok","timestamp":1748236138030,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"},"user_tz":-540},"id":"mzUeofvVQYt_","outputId":"0b5d8dd9-7370-4ce7-f6d4-e0c4360fdc60"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import math\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.utils as vutils\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1748236138040,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"},"user_tz":-540},"id":"at0YDDxUQ2Y-"},"outputs":[],"source":["data_root = '/content/drive/MyDrive/2025_1_Colab/DeepLearning/Lib/SRGAN'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9345,"status":"ok","timestamp":1748236147386,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"},"user_tz":-540},"id":"-NFUPQdBQg95"},"outputs":[],"source":["nz = 100\n","ngf = 128\n","ndf = 128\n","nc = 3\n","img_size = 64\n","transform = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.CenterCrop(img_size),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","batch_size = 512\n","\n","train_dataset = datasets.ImageFolder(\n","    root = data_root,\n","    transform = transform\n",")\n","\n","train_loader = DataLoader(\n","    train_dataset, batch_size = batch_size, shuffle = True\n",")"]},{"cell_type":"code","source":["# IMGSIZE 64일때 실행\n","class Generator(nn.Module):\n","    def __init__(self, nz, ngf, nc, img_size): # img_size는 참고용일 수 있음\n","        super(Generator, self).__init__()\n","        self.nz = nz\n","        self.ngf = ngf\n","        self.nc = nc\n","        # self.img_size = img_size # 직접적인 레이어 크기 결정에는 사용 안 할 수도 있음\n","\n","        self.main = nn.Sequential(\n","            # 입력: nz (잠재 벡터)\n","            # 첫 번째 ConvTranspose2d: nz를 (ngf * 8) 채널의 4x4 특징 맵으로 변환\n","            # 출력: (ngf * 8) x 4 x 4\n","            nn.ConvTranspose2d(nz, ngf * 8, kernel_size=4, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","\n","            # 두 번째 ConvTranspose2d: 4x4 -> 8x8\n","            # 입력: (ngf * 8) x 4 x 4\n","            # 출력: (ngf * 4) x 8 x 8\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","\n","            # 세 번째 ConvTranspose2d: 8x8 -> 16x16\n","            # 입력: (ngf * 4) x 8 x 8\n","            # 출력: (ngf * 2) x 16 x 16\n","            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","\n","            # 네 번째 ConvTranspose2d: 16x16 -> 32x32\n","            # 입력: (ngf * 2) x 16 x 16\n","            # 출력: ngf x 32 x 32\n","            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","\n","            # 다섯 번째 ConvTranspose2d: 32x32 -> 64x64 (최종 이미지 크기)\n","            # 입력: ngf x 32 x 32\n","            # 출력: nc x 64 x 64\n","            nn.ConvTranspose2d(ngf, nc, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh()  # 출력 픽셀 값을 [-1, 1] 범위로\n","        )\n","\n","    def forward(self, input):\n","        # 입력 input은 (batch_size, nz) 형태의 2D 텐서\n","        # 이를 (batch_size, nz, 1, 1) 형태의 4D 텐서로 변환하여 ConvTranspose2d에 전달\n","        return self.main(input.view(-1, self.nz, 1, 1))\n","class Discriminator(nn.Module):\n","    def __init__(self, nc, ndf, img_size): # img_size는 참고용일 수 있음\n","        super(Discriminator, self).__init__()\n","        self.nc = nc\n","        self.ndf = ndf\n","        # self.img_size = img_size # 직접적인 레이어 크기 결정에는 사용 안 할 수도 있음\n","\n","        self.main = nn.Sequential(\n","            # 입력: nc x 64 x 64\n","            # 첫 번째 Conv2d: 64x64 -> 32x32\n","            # 출력: ndf x 32 x 32\n","            nn.Conv2d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True), # 첫 레이어에는 BatchNorm 보통 생략\n","\n","            # 두 번째 Conv2d: 32x32 -> 16x16\n","            # 입력: ndf x 32 x 32\n","            # 출력: (ndf * 2) x 16 x 16\n","            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # 세 번째 Conv2d: 16x16 -> 8x8\n","            # 입력: (ndf * 2) x 16 x 16\n","            # 출력: (ndf * 4) x 8 x 8\n","            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # 네 번째 Conv2d: 8x8 -> 4x4\n","            # 입력: (ndf * 4) x 8 x 8\n","            # 출력: (ndf * 8) x 4 x 4\n","            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # 다섯 번째 Conv2d: 4x4 특징맵을 1x1 스칼라 값으로\n","            # 입력: (ndf * 8) x 4 x 4\n","            # 출력: 1 x 1 x 1\n","            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=0, bias=False),\n","            nn.Sigmoid()  # 출력을 0과 1 사이의 확률 값으로\n","        )\n","\n","    def forward(self, input):\n","        # 입력 input은 (batch_size, nc, 64, 64) 형태의 이미지 텐서\n","        # 최종 출력은 (batch_size, 1) 형태가 되도록 view 조정\n","        return self.main(input).view(-1, 1)"],"metadata":{"id":"H41KRF_WHtOA","executionInfo":{"status":"ok","timestamp":1748236147401,"user_tz":-540,"elapsed":4,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBdTWXD7R9S6"},"outputs":[],"source":["# IMGSIZE 32일때 실행\n","class Generator(nn.Module):\n","    def __init__(self, nz, ngf ,nc, img_size):\n","        super(Generator, self).__init__()\n","        self.nz = nz\n","        self.ngf = ngf\n","        self.nc = nc\n","        self.img_size = img_size\n","\n","        self.main = nn.Sequential(\n","            nn.ConvTranspose2d(nz, ngf * 4, kernel_size = 4, stride = 1, padding = 0, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size = 4, stride = 2, padding = 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size = 4, stride = 2, padding = 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(ngf, nc, kernel_size = 4, stride = 2, padding = 1, bias=False),\n","            nn.Tanh()\n","        )\n","    def forward(self, input):\n","        return self.main(input.view(-1, self.nz, 1, 1))\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nc, ndf, img_size): # nc, img_size 추가\n","        super(Discriminator, self).__init__()\n","        self.nc = nc\n","        self.img_size = img_size\n","        self.ndf = ndf\n","        self.main = nn.Sequential(\n","            nn.Conv2d(nc, ndf, kernel_size = 4, stride = 2, padding = 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(ndf, ndf * 2, kernel_size = 4, stride = 2, padding = 1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(ndf * 2, ndf * 4, kernel_size = 4, stride = 2, padding = 1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(ndf * 4, 1, kernel_size = 4, stride = 1, padding = 0, bias=False),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, input):\n","      return self.main(input).view(-1, 1)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":426,"status":"ok","timestamp":1748236147828,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"},"user_tz":-540},"id":"I4qJ31qZk0OE","outputId":"acdac1df-db61-4753-c361-f09d1310b8d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generator(\n","  (main): Sequential(\n","    (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (13): Tanh()\n","  )\n",")\n","Discriminator(\n","  (main): Sequential(\n","    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (12): Sigmoid()\n","  )\n",")\n"]}],"source":["generator = Generator(nz, ngf, nc, img_size).to(device) # nc, image_sz 전달\n","discriminator = Discriminator(nc, ndf, img_size).to(device) # nc, image_sz 전달\n","print(generator)\n","print(discriminator)"]},{"cell_type":"code","source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)"],"metadata":{"id":"PBDRt0SpQpN7","executionInfo":{"status":"ok","timestamp":1748236147831,"user_tz":-540,"elapsed":2,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["generator.apply(weights_init)\n","discriminator.apply(weights_init)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNb5ULLtQw5p","executionInfo":{"status":"ok","timestamp":1748236147836,"user_tz":-540,"elapsed":4,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"}},"outputId":"d4805dca-d2f4-4add-ee73-b3835b96b7ab"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discriminator(\n","  (main): Sequential(\n","    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (12): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1748236147852,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"},"user_tz":-540},"id":"o7tYGf1bk5fD"},"outputs":[],"source":["optim_g = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n","optim_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","criterion = nn.MSELoss()\n","\n","losses_g = [] # 생성자 오차 저장\n","losses_d = [] # 판별자 오차 저장\n","images = []"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1748236147854,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"},"user_tz":-540},"id":"x7SHmKvuk6Q-"},"outputs":[],"source":["def train_discriminator(optimizer, data_real, data_fake_detached, criterion, device):\n","    b_size = data_real.size(0)\n","\n","    smooth_real = 0.9\n","    smooth_fake = 0.0\n","\n","    real_label = torch.full((b_size, 1), smooth_real, dtype=torch.float, device = device)\n","    fake_label = torch.zeros(b_size, 1, dtype=torch.float, device=device)\n","\n","    optimizer.zero_grad()\n","\n","    output_real = discriminator(data_real)\n","    loss_real = criterion(output_real, real_label)\n","\n","    output_fake = discriminator(data_fake_detached)\n","    loss_fake = criterion(output_fake, fake_label)\n","\n","    loss_total = (loss_real + loss_fake)/2\n","    loss_total.backward()\n","    optimizer.step()\n","    return loss_total.item()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1748236147855,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"},"user_tz":-540},"id":"U-0RhiBWk7vS"},"outputs":[],"source":["def train_generator(optimizer, data_fake, criterion, device):\n","    b_size = data_fake.size(0)\n","    real_label = torch.ones(b_size, 1).to(device)\n","\n","    optimizer.zero_grad()\n","    output = discriminator(data_fake)\n","\n","    loss = criterion(output, real_label)\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1CY13IKfN2h3DcbYPL0aJRl5bd_LPZZZX"},"id":"pqVpLFnMk9RI","executionInfo":{"status":"error","timestamp":1748240770628,"user_tz":-540,"elapsed":311477,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"}},"outputId":"7572923d-309b-4631-d391-893abdbbc714"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["num_samples_to_show = 16 # 4x4 그리드로 표시할 이미지 수\n","fixed_noise = torch.randn(num_samples_to_show, nz, 1, 1, device=device) # DCGAN 입력 형태\n","\n","epochs = 1000\n","for epoch in range(epochs):\n","    loss_g_sum = 0.0\n","    loss_d_sum = 0.0\n","    num_batches = 0 # 변수명 수정: num_batchs -> num_batches\n","\n","    generator.train() # 학습 모드 설정\n","    discriminator.train() # 학습 모드 설정\n","\n","    for idx, data in enumerate(train_loader):\n","        image, _ = data\n","        data_real_batch = image.to(device)\n","        current_batch_size = data_real_batch.size(0)\n","\n","        if current_batch_size == 0: continue\n","\n","        noise_for_d = torch.randn(current_batch_size, nz, 1, 1, device=device) # FUCK\n","        data_fake_for_d = generator(noise_for_d).detach()\n","        loss_d_batch = train_discriminator(optim_d, data_real_batch, data_fake_for_d, criterion, device)\n","        loss_d_sum += loss_d_batch\n","\n","        noise_for_g = torch.randn(current_batch_size, nz, 1, 1, device=device) # DCGAN 생성자 입력 형태\n","        data_fake_for_g = generator(noise_for_g)\n","        loss_g_batch = train_generator(optim_g, data_fake_for_g, criterion, device)\n","        loss_g_sum += loss_g_batch\n","\n","        num_batches += 1 # 여기서 num_batches 증가\n","\n","    if num_batches > 0:\n","        avg_epoch_loss_g = loss_g_sum / num_batches\n","        avg_epoch_loss_d = loss_d_sum / num_batches\n","    else:\n","        avg_epoch_loss_g = 0\n","        avg_epoch_loss_d = 0\n","        print(f\"경고: 에포크 {epoch}에서 처리된 배치가 없습니다.\")\n","\n","    losses_g.append(avg_epoch_loss_g) # 리스트에 추가\n","    losses_d.append(avg_epoch_loss_d) # 리스트에 추가\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch}/{epochs}\") # epochs는 총 에포크 수\n","        print(f\"Avg Generator loss: {avg_epoch_loss_g:.4f}, Avg Discriminator loss: {avg_epoch_loss_d:.4f}\")\n","\n","    if (epoch + 1) % 100 == 0 or epoch == epochs - 1: # 첫 에포크, 매 100 에포크, 마지막 에포크\n","        # 생성된 이미지 시각화\n","        generator.eval() # 평가 모드로 전환\n","        with torch.no_grad(): # 그래디언트 계산 비활성화\n","            fake_images_sample = generator(fixed_noise).detach().cpu()\n","\n","            # Matplotlib으로 플롯\n","            grid_img = vutils.make_grid(fake_images_sample, nrow=int(math.sqrt(num_samples_to_show)), padding=2, normalize=True)\n","            plt.figure(figsize=(8, 8)) # 그림 크기\n","            plt.axis(\"off\")\n","            plt.title(f\"Generated Fake Images at Epoch {epoch+1}\")\n","            plt.imshow(np.transpose(grid_img, (1, 2, 0))) # C, H, W -> H, W, C\n","\n","            plt.show() # Colab 등에서 바로 이미지 보기\n","\n","        generator.train() # 다시 학습 모드로 전환"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeJYg4_hCT6B"},"outputs":[],"source":["plt.figure()\n","plt.plot(torch.Tensor(losses_g), label = 'Generator loss')\n","plt.plot(torch.Tensor(losses_d), label = 'Discriminator Loss')\n","plt.legend()"]},{"cell_type":"code","source":["num_images_to_save = 2  # 저장할 가짜 이미지의 개수\n","output_dir = 'individual_fake_images' # 이미지를 저장할 폴더 이름\n","os.makedirs(output_dir, exist_ok=True) # 폴더가 없으면 생성\n","\n","# 모델을 평가 모드로 설정\n","generator.eval()\n","\n","# 그래디언트 계산 비활성화\n","with torch.no_grad():\n","    # 저장할 개수만큼의 노이즈 생성\n","    # 각 이미지는 서로 다른 노이즈로부터 생성되도록 함\n","    noise = torch.randn(num_images_to_save, nz, 1, 1, device=device) # DCGAN 입력 형태\n","\n","    # 생성자로부터 가짜 이미지 생성\n","    fake_images_batch = generator(noise).detach().cpu() # (num_images_to_save, nc, img_size, img_size) 형태\n","\n","    # --- 생성된 가짜 이미지를 개별 파일로 저장 ---\n","    for i in range(num_images_to_save):\n","        # fake_images_batch[i]는 단일 이미지 텐서 (nc, img_size, img_size)\n","        # PyTorch의 save_image는 (C, H, W) 또는 (B, C, H, W) 형태를 받으므로,\n","        # 단일 이미지를 저장할 때는 (C, H, W)로 전달하거나,\n","        # 배치 차원을 유지한 채 (1, C, H, W)로 전달하고 nrow=1로 설정할 수 있습니다.\n","        # 여기서는 간단하게 단일 이미지 텐서를 직접 전달합니다.\n","\n","        image_tensor_to_save = fake_images_batch[i] # (nc, img_size, img_size)\n","\n","        # 파일 경로 설정 (예: fake_image_1.png, fake_image_2.png)\n","        file_path = os.path.join(output_dir, f\"fake_image_{i+1}.png\")\n","\n","        # 단일 이미지 저장\n","        # normalize=True: 픽셀 값을 [0, 1] 범위로 정규화 (Tanh 출력 [-1,1]을 고려)\n","        vutils.save_image(image_tensor_to_save, file_path, normalize=True)\n","\n","        print(f\"가짜 이미지 {i+1} 저장 완료: {file_path}\")\n","\n","\n","# (선택 사항) 저장된 이미지 중 하나를 Matplotlib으로 바로 확인하고 싶다면:\n","# if num_images_to_save > 0:\n","#     # 첫 번째로 저장된 이미지 (또는 원하는 이미지)를 불러와서 표시\n","#     # (위에서 fake_images_batch[0]을 사용해도 됨)\n","#     img_to_show = fake_images_batch[0] # 이미 CPU에 있는 상태\n","#     plt.figure(figsize=(4, 4)) # 그림 크기\n","#     plt.axis(\"off\")\n","#     plt.title(\"One of the Generated Fake Images\")\n","#     plt.imshow(np.transpose(img_to_show.numpy(), (1, 2, 0))) # C, H, W -> H, W, C\n","#     plt.show()\n","\n","print(f\"\\n총 {num_images_to_save}개의 가짜 이미지가 '{output_dir}' 폴더에 저장되었습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WH7twhozbUjY","executionInfo":{"status":"ok","timestamp":1748032973199,"user_tz":-540,"elapsed":4,"user":{"displayName":"이봉규 (opado)","userId":"06890509259234494548"}},"outputId":"aeac3267-034b-4c5f-8a39-150d13724dae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["가짜 이미지 1 저장 완료: individual_fake_images/fake_image_1.png\n","가짜 이미지 2 저장 완료: individual_fake_images/fake_image_2.png\n","\n","총 2개의 가짜 이미지가 'individual_fake_images' 폴더에 저장되었습니다.\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1a9PLgFQpeOLh6eZ0iWKHeDGQfTNXve7N","authorship_tag":"ABX9TyN+DG/WTPh8qJV2gnU+YIpn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}